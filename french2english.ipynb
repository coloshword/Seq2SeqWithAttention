{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch from scratch \n",
    "\n",
    "- we will use one-hot vector, giant vector of zeros, except for a single one\n",
    "- define a helper class Lang "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import unicodedata\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch import optim\n",
    "import re \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "    \n",
    "    # split the sentence, and then add the word to the word2index dictionary \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    # addWord: adds the word to the dict, if they don't exist in the dictionary \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a convert to ascii because engglish and french kind of save an alphabet, and it would allow us to save\n",
    "# some characters \n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "## normalize, make all strings lowercase and trim, removing non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # find punctuation, and split it\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # remove non-letter characters  \n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lines, and create languages \n",
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    lines = []\n",
    "    # read the file and split into lines \n",
    "    with open(\"eng-fra.txt\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        # normalize string\n",
    "        # for each line, we split it into an array [eng, french] using .split()\n",
    "        lines = [[normalize_string(s) for s in l.split('\\t')] for l in lines] \n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    return input_lang, output_lang, lines \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train something quickly, trim the data set to only those who are short and simple \n",
    "# max length is 10 words, and only sentences that translate to the form \"I am / He is, account for apostrophes and other thing s\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "# returns true if less than MAX_LENGTH and prefix is the correct one \n",
    "def filter_pair(p):\n",
    "    filter = MAX_LENGTH > len(p[0].split(' '))\n",
    "    starts_with_filter = p[0].startswith(eng_prefixes)\n",
    "\n",
    "    return filter and starts_with_filter\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Reading 135842 sentence pairs\n",
      "trimmed to 11492 sentence pairs \n",
      "counting words\n",
      "Counted words:\n",
      "eng 2978\n",
      "fra 4597\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1, lang2, reverse)\n",
    "    print(\"Reading %s sentence pairs\" % len(pairs))\n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"trimmed to %s sentence pairs \" % len(pairs))\n",
    "\n",
    "    print(\"counting words\")\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs \n",
    "\n",
    "prepare_data('eng', 'fra', True)\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder \n",
    "- in this case we're doing word level tokenization. \n",
    "\n",
    "- encoder consists of:\n",
    "    - Embedding layer (turning intput tokens into meaning vector)\n",
    "    - For each token GRU layer (RNN) portion, takes in embedded input vector, outputs an output vector, as well as a hidden state. In RNN fashion, the vector \n",
    "    - hidden state is used in the next run so ultimate GRU layer outputs N output vectors, \n",
    "    where N is the number of tokens, and ultimately one last hidden state. \n",
    "\n",
    "\n",
    "Encoder():\n",
    "    input_size **size of vocab**\n",
    "    hidden_size **dimension of hidden states** \n",
    "    dropout rate **rate at which the neurons will output a 0 instead of their regular output**\n",
    "    **dropout rate of 0.1 means that for every 10 neurons, 1 neuron is going to output 0 regardless of its desired output** \n",
    "    EMBEDDING LAYER ( nn.Embedding(output_size, hidden_size))\n",
    "    \n",
    "neuron just is an element in the vector. For example, embedding layer which is represented by a vector (e.g. 300 x 1), each of the elements in this vector is a neuron\n",
    "- when you apply the dropoout to embeddings, we randomly set some of the elements of the embedding layer to 0, for the purpose of preventing overfitting in nn \n",
    "\n",
    "Dropout example:\n",
    "# create embedding layer of 1000 words, and embedding dimension of 5 \n",
    "embedding = nn.Embedding(1000, 5)\n",
    "dropout = nn.Dropout(p=0.4)\n",
    "\n",
    "# input: indices of two words \n",
    "- we input the indices of two words \n",
    "torch.tensor([42, 123])\n",
    "- think about this, we have 2 words, and we are applying embedding to them. Embedding has 5 dimensions, so you get a 2 x 5 tensor, where each 1 x 5 tensor represents the embedding of the word\n",
    "emb_output = embedding(input_indices)\n",
    "result:\n",
    "[[\n",
    "    0.123, 0.23, -0.23, 0.567, -0.9\n",
    "    0.243, 0.555, 0.666, -0.999, 0.77\n",
    "]]\n",
    "\n",
    "**each of these numbers is a neuron! So when we apply dropout to it, we expect 40% of the neurons to turn to 0, but since each word is 5 neurons we are not dropping the word, but more like dropping some features of the word**\n",
    "\n",
    "output might look like this:\n",
    "[[\n",
    "    0.000, 0.23, 0.000, 0.567, -0.9\n",
    "    0.243, 0.000, 0.666, -0.999, 0.00\n",
    "]]\n",
    "\n",
    "**Embedding layer**\n",
    "- firstly, an embedding is a mapping. Maps integer indices to dense vectors. \n",
    "\n",
    "- when creating an embedding layer, you specify 2 main parameters: \n",
    "1) number of embeddings\n",
    "2) embedding dimension \n",
    "\n",
    "- embedding layer creates a weight matrix of shape (num_embeddings, embedding_dim)\n",
    "- EMBEDDING layer is just one big matrix. This is how it acts as a dictionary mapping:\n",
    "word 0: vector 0\n",
    "word 1: vector 1\n",
    "word 2: vector 2\n",
    "\n",
    "- so it is a matrix of (num_words, vector dim)\n",
    "- to get the vector representing the nthword, you choose the nth row in the matrix \n",
    "- These vectors are called **dense vectors**, and the way they are created is that originally the vectors are initialized randomly, using the Embedding() class \n",
    "- DURING BACKPROP, these layers are also updated!, so for the RNN learning process this layer is also updated to minimize loss. \n",
    "- That's how the embedding layer is able to map the meaning of a word, it learns from the meaning of the word from the training \n",
    "\n",
    "**KEY TAKEAWAY**: embedding layer is a mapping of word index to dense vector. Dense vector changes to minimize loss during backprop. \n",
    "\n",
    "**GRU layer**\n",
    "- RNN architecture, used to **capture** dependencies, which I guess work by the addition of the hidden state at each level. \n",
    "\n",
    "- creation: \n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "- the gru layer has input and hidden state, hidden_size. \n",
    "- gru processes input sequence one element at a time. Outputs a sequence of hidden states (one for each input time state) and the final hidden state  \n",
    "\n",
    "**Forward pass**\n",
    "def forward(self, input):\n",
    "    embedded = self.dropout(self.embedding(input))\n",
    "    output, hidden = self.gru(embedded)\n",
    "    return output, hidden\n",
    "\n",
    "- IMPORTANT: input is NOT one token, it is the entire input sequence. So for an input sentence, we don't just pass one token at a time, the forward pass handles the entire input sentence.\n",
    "\n",
    "- **what is input**\n",
    "\n",
    "- input is a matrix: \n",
    "    (batch size x sequence_length)\n",
    "    - batch_size is equal to the number of sentences in input\n",
    "    - so input captures the sentences, and the length of each sentence \n",
    "\n",
    "    - so how does this get the embedding vector for each word in the sentence?\n",
    "    - input:\n",
    "s0    w0 w1 w2 w3 w 4 \n",
    "s1    w0 w1 w2 w3 w 4 \n",
    "s2    w0 w1 w2 w3 w 4 \n",
    "23    w0 w1 w2 w3 w 4   \n",
    "\n",
    "    - so basically the input is a bunch of sentences, and the word for each sentence\n",
    "    - the word for each sentence is acutally just the INDEX of the word corresponding to the embedding layer\n",
    "    - so for each index, we can pull the dense vector that corresponds to that word from the embedding layer! \n",
    "\n",
    "**Key takeaways**\n",
    "SINGLE step of GRU'S internal processing:\n",
    "(input) --> embedding_layer --> (embedded vector ) + prev_hidden --> gru --> output, hidden\n",
    "\n",
    "While forward pass is the entire process! Multiple of these steps \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "- decoder is just another RNN.\n",
    "Input: Encoder's output vectors \n",
    "Output: Sequence of words that represents the translation \n",
    "\n",
    "- RNN, so every step it takes:\n",
    "input: input token, hidden state\n",
    "**Initial input token is <sos> token and the hidden state is actually the CONTEXT vector, the encoder's last hidden state**\n",
    "\n",
    "(input) --> embedding --> relu(embedding) + prev_hidden --> gru \n",
    "\n",
    "hidden + gru --> out --> softmax --> translated token! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "    \n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "        \n",
    "        decoder_ouputs = torch.cat()\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None \n",
    "\n",
    "    # one forward step \n",
    "    # remember, one input --> embedding --> relu + prev_hidden --> GRU --> softmax(output), hidden \n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention decoder\n",
    "- problem with current set up is that we only pass the last hidden state\n",
    "- this single vector has to encode the entire sequence. Attention allows the decoder network to \"focus\" on a different part of the encoder's outputs for every step of the decoder's own outputs. \n",
    "- calculate the attention weights. With the attention weights, we multiply then by the encoder output vectors, to create a weighted combination. The result will contain information about that specific part of the input sequence, and thus help the decoder choose the right output words \n",
    "\n",
    "- calculating the attention weights is done with another feed-forward layer attn, using the decoder's input and hidden state as inputs. Because there are sentences of all sizes in the training data, to actually create and train this layer we have to choose a maximum sequence length that it can apply to. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few \n",
    "\n",
    "- we'll do this after"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
